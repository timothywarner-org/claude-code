# Memory MCP Server

A FastMCP server for storing and retrieving project notes, PRDs, and code context with DeepSeek-powered optimization for token-efficient retrieval. Built for the "Claude Code with Large-Context Reasoning" course.

## Features

- ğŸ“ **Memory Management** â€” Store notes, PRDs, code snippets, decisions, patterns, configs, and troubleshooting guides
- ğŸ·ï¸ **Smart Tagging** â€” 7 predefined course-relevant tags for easy categorization
- ğŸ” **Powerful Search** â€” Find memories by keyword, tag, type, or project
- ğŸ¤– **DeepSeek Optimization** â€” Condense memories for token-efficient context injection
- ğŸ’¾ **Intelligent Caching** â€” 1-hour TTL cache to avoid redundant API calls
- ğŸ¯ **7 Memory Types** â€” Note, PRD, snippet, decision, pattern, config, troubleshooting
- ğŸ“Š **Statistics** â€” Track usage by tag, type, and project
- ğŸ”Œ **API Testing** â€” Built-in connectivity tests for DeepSeek and GitHub

## Quick Start

### 1. Setup Environment

```powershell
# Windows (PowerShell 7+)
cd src
pwsh .\setup.ps1
```

```bash
# macOS/Linux
cd src
chmod +x setup.sh
./setup.sh
```

This creates a `.venv` virtual environment and installs all dependencies including:
- `fastmcp` â€” FastMCP server framework
- `python-dotenv` â€” Environment variable loading
- `openai` â€” DeepSeek API client (OpenAI-compatible)
- `tiktoken` â€” Token counting
- `pydantic` â€” Data validation

### 2. Configure Environment Variables

The server loads API credentials from `.env` (already configured):

```env
# DeepSeek (OpenAI-compatible API)
OPENAI_API_KEY=your-deepseek-key
OPENAI_BASE_URL=https://api.deepseek.com
LLM_MODEL=deepseek-chat

# GitHub API
GITHUB_TOKEN=your-github-token
```

### 3. Run the Server

```bash
python memory_server.py
```

The server runs via stdio transport (standard MCP protocol).

### 4. Test with MCP Inspector

```bash
python start_inspector.py
```

Opens a web UI for interactive testing at `http://localhost:[random-port]`.

### 5. Run Tests

```bash
pytest test_memory_server.py -v
```

All tests should pass (includes mocked DeepSeek/GitHub API tests).

## Memory Server Features

### Resources

| Resource | Description |
|----------|-------------|
| `memory://items` | List all memory items with metadata |
| `memory://tags` | List available predefined tags |
| `memory://types` | List all 7 memory types |
| `memory://stats` | Statistics by type, tag, and project |

### Tools

| Tool | Parameters | Description |
|------|------------|-------------|
| `add_memory` | title, content, type, tags[], project | Create new memory |
| `get_memory` | memory_id | Retrieve specific memory |
| `search_memory` | search_term, tag?, type?, project? | Search with filters |
| `get_optimized_memory` | memory_id, max_tokens?, use_cache? | Get token-optimized memory |
| `update_memory` | memory_id, title?, content?, tags?, project? | Update existing memory |
| `delete_memory` | memory_id | Delete memory (in-memory) |
| `list_by_tag` | tag | Get all memories with tag |
| `list_by_type` | type | Get all memories of type |
| `reset_memory` | (none) | Restore deleted memories |
| `test_apis` | (none) | Test DeepSeek/GitHub connectivity |

### Prompts

| Prompt | Parameters | Description |
|--------|------------|-------------|
| `code_review_context` | language, framework?, conventions? | Generate code review context |
| `refactoring_plan` | language, target_issues? | Create refactoring plan |
| `mcp_tool_design` | tool_purpose, tool_name? | Design MCP tool |
| `codebase_analysis` | project_type, focus_areas? | Analyze complete codebase |

## DeepSeek Optimization

### How It Works

When you call `get_optimized_memory`, the server:

1. **Retrieves** the memory content
2. **Counts tokens** using tiktoken (cl100k_base encoding)
3. **Checks cache** for previously optimized version (1-hour TTL)
4. **Calls DeepSeek** to condense content if not cached
5. **Preserves** all technical details:
   - Code snippets (exact syntax)
   - Commands (exact syntax)
   - Config values (API keys, URLs, paths)
   - File names and paths
   - Numbers and version strings
6. **Removes** verbosity:
   - Redundant explanations
   - Conversational filler
   - Unnecessary context
7. **Falls back** to raw content if optimization fails

### Token Efficiency Example

```python
# Original memory: 2500 tokens
result = get_optimized_memory("mem-003", max_tokens=1500)

# Optimized: ~1450 tokens (42% reduction)
# All code blocks, commands, and configs preserved
# Technical accuracy maintained
```

### Cache Benefits

- **Avoids redundant API calls** for frequently accessed memories
- **Reduces cost** (DeepSeek charges per token)
- **Faster retrieval** (no API latency on cache hit)
- **1-hour TTL** ensures fresh content periodically

## Memory Types & Tags

### 7 Memory Types

1. **note** â€” General project notes and observations
2. **prd** â€” Product requirements documents and specifications
3. **snippet** â€” Code snippets and implementation examples
4. **decision** â€” Architecture decisions and technical choices
5. **pattern** â€” Design patterns and best practices
6. **config** â€” Configuration files and settings
7. **troubleshooting** â€” Error solutions and debugging guides

### 7 Predefined Tags

1. **mcp-server** â€” Model Context Protocol development
2. **claude-api** â€” Claude API usage and optimization
3. **tool-use** â€” Function calling and tool schemas
4. **context-window** â€” Large context strategies
5. **production-workflow** â€” CI/CD and automation
6. **debugging** â€” Troubleshooting and solutions
7. **cost-optimization** â€” Token budgets and efficiency

## Sample Data

The server includes 10 realistic course memories:

1. MCP Memory Server architecture patterns
2. PR Review Bot configuration
3. Legacy refactoring strategies
4. Cost management and model selection
5. MCP tool implementation patterns
6. GitHub Actions CI/CD workflows
7. Context window usage best practices
8. MCP server registration and debugging
9. Streaming response implementation
10. Capstone project requirements

## VS Code Integration

### Debug Configuration

Press `F5` and select "ğŸš€ Run MCP Server" to debug.

### Tasks

`Ctrl+Shift+P` â†’ "Tasks: Run Task" â†’ choose:
- Setup Environment (pwsh/bash)
- Run Memory Server
- Test Memory Server
- Start Inspector

### Auto-Activate venv

VS Code automatically activates the venv in new terminals. If not:

1. `Ctrl+Shift+P` â†’ "Python: Select Interpreter"
2. Choose `src/.venv/Scripts/python.exe`
3. Reload window

## Project Structure

```
src/
â”œâ”€â”€ memory_server.py            # Main FastMCP server
â”œâ”€â”€ api_utils.py                # DeepSeek & GitHub API clients
â”œâ”€â”€ test_memory_server.py       # Pytest suite
â”œâ”€â”€ start_inspector.py          # Inspector launcher
â”œâ”€â”€ setup.ps1 / setup.sh        # Environment setup
â”œâ”€â”€ pyproject.toml              # Python dependencies
â”œâ”€â”€ .env                        # API credentials (gitignored)
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ data/
    â””â”€â”€ memory_items.json       # Memory database (10 samples)
```

## Usage Examples

### Adding a Memory

```python
add_memory(
    title="API Rate Limiting Strategy",
    content="Implement exponential backoff with jitter...",
    type="pattern",
    tags=["claude-api", "cost-optimization"],
    project="capstone"
)
```

### Searching Memories

```python
# Find all MCP-related memories
search_memory("tool registration", tag="mcp-server")

# Find troubleshooting guides
search_memory("error", type="troubleshooting")

# Find project-specific memories
search_memory("review", project="capstone")
```

### Optimized Retrieval

```python
# Get condensed version (default 1500 tokens)
get_optimized_memory("mem-007")

# Custom token budget
get_optimized_memory("mem-010", max_tokens=1000)

# Skip cache (force fresh optimization)
get_optimized_memory("mem-003", use_cache=False)
```

### Using Prompts

```python
# Generate code review context
code_review_context(
    language="python",
    framework="FastMCP",
    conventions="Use type hints, max 50 LOC per function"
)

# Create refactoring plan
refactoring_plan(
    language="typescript",
    target_issues="God class, missing error handling"
)
```

## API Connectivity Testing

Test DeepSeek and GitHub API connections:

```python
test_apis()
# Returns:
# {
#   "deepseek": {"connected": true, "message": "Connection successful"},
#   "github": {"connected": true, "message": "Connected as: username"},
#   "cache_stats": {"total_entries": 5, "avg_reduction": "45.2%"}
# }
```

## Troubleshooting

### "OPENAI_API_KEY not found"

Ensure `.env` file exists in `src/` directory with valid DeepSeek API key.

### "Module not found" errors

Run setup script to create venv and install dependencies:
```bash
pwsh ./setup.ps1  # Windows
./setup.sh        # macOS/Linux
```

### Optimization fails

Server falls back to raw content automatically. Check DeepSeek API status and key validity with `test_apis()` tool.

### Port already in use (Inspector)

`start_inspector.py` automatically finds an available port. If issues persist, close other inspector instances.

## Requirements

- Python 3.10+
- Node.js 18+ (for MCP Inspector)
- PowerShell 7+ (Windows) or Bash (Unix)
- DeepSeek API key (optional, optimization gracefully degrades)
- GitHub token (optional, for future integrations)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     stdio      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP Inspector  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   memory_server.py   â”‚
â”‚  (Web UI)       â”‚                â”‚   (FastMCP/Python)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚   api_utils.py       â”‚
                                   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                                   â”‚   â”‚ DeepSeekClientâ”‚   â”‚
                                   â”‚   â”‚   + Cache    â”‚   â”‚
                                   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                                   â”‚   â”‚ GitHubClient â”‚   â”‚
                                   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚  data/memory_items   â”‚
                                   â”‚  .json (10 samples)  â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## License

MIT
